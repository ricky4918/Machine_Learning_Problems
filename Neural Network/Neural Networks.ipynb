{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Networks.ipynb","provenance":[],"authorship_tag":"ABX9TyMiZSSURAv+JEOHolQjtYE9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["A Denely Connected Neural network is one in which all the neurons in the current layer are connected to every neuron in the previous layer\n","\n","n1 = sum(WiXi) +b"],"metadata":{"id":"nxib_UqCIgVR"}},{"cell_type":"code","source":[""],"metadata":{"id":"g7SDvV-7N3ud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Activation Functions\n","\n","- Relu(Rectified Linear Unit) : any value below 0 is 0\n","- Tanh(hyperbolic Tangent) : switches values between -1 and 1\n","- Sigmoid (o or 1)\n","\n","\n","N1 = F(sum(WiXi) +b)\n","\n","\n","# lost/cost functions\n","mean suqared Error\n","mean absolute error\n","Hinge loss\n","\n","\n","# Gradient Descent\n","to find the optimal parameter for our network, while backpropagatin is the process of calculating the gradient that is used in the gradient descent step\n","\n","# Optimizer\n","\n","function that implements the backpropagation algorithm \n","- Gradient Descent\n","- Stochastic gradient Descent\n","- Mini-Batch Gradient Descent\n","- Momentum\n","- Nesterov Accelerated Gradient\n","\n"],"metadata":{"id":"Ya_mwSFfIggi"}},{"cell_type":"code","source":[""],"metadata":{"id":"qewOqmePR_qN"},"execution_count":null,"outputs":[]}]}